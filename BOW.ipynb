{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03MjsWDWgNjK"
      },
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eT4bJvCi5h5E"
      },
      "source": [
        "<html>\n",
        "<Head>\n",
        "<h1 style=\"color:blue; align:center\"> Can your Machine be Human enough to Understand your Language?</h1>\n",
        "<h2>Fahim bhaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaai</h2>\n",
        "<style>\n",
        "h1 { color : skyblue; align : center}\n",
        "</style>\n",
        "</Head>\n",
        "</html>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjO5REtrJFv4"
      },
      "source": [
        "<html>\n",
        "<head>\n",
        "<h1 align=center><b><u>Bag of Words</b></u><h1>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "> A model for coverting language into vectors to use as input data to perform multiple programming tasks such as classification, similarity detection etc. \n",
        "The numbers in the converted vector is the logical representation of the words.\n",
        "\n",
        "BOW Steps: \n",
        "- Preprocessing the data \n",
        "    - Removing punctuations \n",
        "    - Turning the entire corpus into lowercase\n",
        "    - Removing stop words \n",
        "- Vocabulary \n",
        "- Word frequency \n",
        "- Vector representation\n",
        "\n",
        "<body>[link text](`https://`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSAd4LiDUYom"
      },
      "source": [
        "# Importing necessary Libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWaCI9wMUUgu"
      },
      "outputs": [],
      "source": [
        "import nltk  \n",
        "import numpy as np  \n",
        "import random  \n",
        "import string\n",
        "\n",
        "import bs4 as bs  \n",
        "import urllib.request  \n",
        "import re  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFwm68f-Ukfs"
      },
      "source": [
        "# The Input Article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IObXLLYFU3QG"
      },
      "outputs": [],
      "source": [
        "#Importing Raw HTML from an Article \n",
        "raw_html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')  \n",
        "raw_html = raw_html.read()\n",
        "\n",
        "#Parse the data\n",
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "#filtering paragraphs\n",
        "article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "article_text = ''\n",
        "\n",
        "#Concatening all paragraphs \n",
        "for para in article_paragraphs:  \n",
        "    article_text += para.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdYWw5iaV0UZ"
      },
      "source": [
        "# Sentence Tokenization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "072KrLNOV3Go"
      },
      "outputs": [],
      "source": [
        "#Splitting the corpus into individual sentences\n",
        "corpus = nltk.sent_tokenize(article_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUEovK3uWmAK"
      },
      "source": [
        "#Turning into lowercase "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pV8JViXoWy6a"
      },
      "outputs": [],
      "source": [
        "for i in range(len(corpus )):\n",
        "    corpus [i] = corpus [i].lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZKZGqlVW0iS"
      },
      "source": [
        "# Removing punctuations using reg ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MYtP9fHW7xR"
      },
      "outputs": [],
      "source": [
        "for i in range(len(corpus))\n",
        "corpus [i] = re.sub(r'\\W',' ',corpus [i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHQ_HSj6XF9e"
      },
      "source": [
        "# Removing empty spaces using reg ex\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLlPAwb5XT0B"
      },
      "outputs": [],
      "source": [
        "for i in range(len(corpus))\n",
        "corpus [i] = re.sub(r'\\W',' ',corpus [i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C3sLiPbXjBy"
      },
      "source": [
        "# Sentence tokenization and word frequency \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwRJH-xmYJX9"
      },
      "outputs": [],
      "source": [
        "wordfreq = {}\n",
        "for sentence in corpus:\n",
        "    #Tokenization\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    #Word frequency\n",
        "    for token in tokens:\n",
        "        if token not in wordfreq.keys():\n",
        "            wordfreq[token] = 1\n",
        "        else:\n",
        "            wordfreq[token] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVeM3CM_YTrp"
      },
      "source": [
        "#Filtering the vocabulary \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6G7SXbfYemZ"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "#Getting 200 of the most frequent words in the corpus \n",
        "most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcvYwDlkYoCO"
      },
      "source": [
        "#Converting the sentences into Vectors "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjGcOrTGY9HD"
      },
      "outputs": [],
      "source": [
        "# Vector of all sentences\n",
        "sentence_vectors = []\n",
        "for sentence in corpus:\n",
        "    sentence_tokens = nltk.word_tokenize(sentence)\n",
        "    #Vector of a sentence\n",
        "    sent_vec = []\n",
        "    for token in most_freq:\n",
        "        if token in sentence_tokens:\n",
        "            sent_vec.append(1)\n",
        "        else:\n",
        "            sent_vec.append(0)\n",
        "    sentence_vectors.append(sent_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYrk6bX3ZX0t"
      },
      "source": [
        "#Converting data into a Matrix \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIiW3wu-ZcgF"
      },
      "outputs": [],
      "source": [
        "sentence_vectors = np.asarray(sentence_vectors)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
